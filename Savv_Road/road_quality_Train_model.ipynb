{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wP5HzzyBe5R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2486e57-9784-452e-bc27-c1d55064c8b1"
      },
      "source": [
        "## Load the required modules\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split, PredefinedSplit, cross_validate\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "!pip install keras-tuner --upgrade\n",
        "import keras_tuner\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "!pip install scikit-optimize --upgrade\n",
        "from skopt import BayesSearchCV\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "import pickle as pkl\n",
        "import collections"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting keras-tuner\n",
            "  Downloading keras_tuner-1.1.2-py3-none-any.whl (133 kB)\n",
            "\u001b[K     |████████████████████████████████| 133 kB 7.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (21.3)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (5.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (1.21.6)\n",
            "Collecting kt-legacy\n",
            "  Downloading kt_legacy-1.0.4-py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.7/dist-packages (from keras-tuner) (2.8.0)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (5.1.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (4.4.2)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (2.6.1)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (57.4.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (1.0.18)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (0.8.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->keras-tuner) (4.8.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->keras-tuner) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->keras-tuner) (0.2.5)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->keras-tuner) (3.0.8)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->keras-tuner) (0.7.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->keras-tuner) (2021.10.8)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.0.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (3.17.3)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.8.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.44.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (1.35.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (3.3.6)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.37.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard->keras-tuner) (0.4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard->keras-tuner) (4.11.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->keras-tuner) (4.2.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->keras-tuner) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->keras-tuner) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (3.2.0)\n",
            "Installing collected packages: kt-legacy, keras-tuner\n",
            "Successfully installed keras-tuner-1.1.2 kt-legacy-1.0.4\n",
            "Collecting scikit-optimize\n",
            "  Downloading scikit_optimize-0.9.0-py2.py3-none-any.whl (100 kB)\n",
            "\u001b[K     |████████████████████████████████| 100 kB 3.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.21.6)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.0.2)\n",
            "Collecting pyaml>=16.9\n",
            "  Downloading pyaml-21.10.1-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-optimize) (1.1.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from pyaml>=16.9->scikit-optimize) (3.13)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->scikit-optimize) (3.1.0)\n",
            "Installing collected packages: pyaml, scikit-optimize\n",
            "Successfully installed pyaml-21.10.1 scikit-optimize-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxo5g9jOKafk",
        "outputId": "f09c6493-28a6-477c-ffed-5dbf92769385"
      },
      "source": [
        "## Mount the google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "## Set the directory\n",
        "########## First, create a shortcut of \"streetviews2/arrs_pkl\" folder in your drive ########\n",
        "%cd /content/drive/MyDrive/streetviews2/arrs_pkl/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n",
            "/content/drive/.shortcut-targets-by-id/1gnVV0eOEygqj9_wnneoTbAKcSf-W4nJS/streetviews2/arrs_pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pL8yXr38d3np"
      },
      "source": [
        "## **Load the imaging Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fN0pYU3IQe-u"
      },
      "source": [
        "## Images directory and their labels\n",
        "img_arr_filename_lst = ['img0_arr.csv', 'img1_arr.csv', 'img2_arr.csv', 'img3_arr.csv', 'img4_arr.csv', 'img5_arr.csv']\n",
        "pci_arr_filename_lst = ['pci0_arr.csv', 'pci1_arr.csv', 'pci2_arr.csv', 'pci3_arr.csv', 'pci4_arr.csv', 'pci5_arr.csv']\n",
        "\n",
        "img = []\n",
        "label = []\n",
        "\n",
        "for i in range(len(img_arr_filename_lst)):\n",
        "  with open(img_arr_filename_lst[i],'rb') as f:\n",
        "    img.append(pkl.load(f).tolist())\n",
        "\n",
        "  with open(pci_arr_filename_lst[i],'rb') as f1:\n",
        "    label.append(pkl.load(f1).tolist())\n",
        "\n",
        "train_images = []\n",
        "train_labels = []\n",
        "\n",
        "for i in img:\n",
        "  for j in i:\n",
        "    train_images.append(j)\n",
        "\n",
        "for i in label:\n",
        "  for j in i:\n",
        "    train_labels.append(j)\n",
        "\n",
        "train_labels[155] = 5\n",
        "\n",
        "X = np.array(train_images)\n",
        "y = np.array(train_labels)\n",
        "\n",
        "## Split the dataset into training, validation, and test sets\n",
        "X, X_test, y, y_test = train_test_split(X, y, test_size=1/3, random_state=0)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=1/2, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_test = []\n",
        "with open('img_first_1000_arr.csv','rb') as f:\n",
        "  img_test.append(pkl.load(f).tolist())\n",
        "\n",
        "len(img_test)"
      ],
      "metadata": {
        "id": "9Y489n0xVidx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aed7e831-a660-48a7-cdf0-bd97ad6a317b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYIdgBZP_B-I"
      },
      "source": [
        "## Model Collection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0vpl61I_frF"
      },
      "source": [
        "## Validation and test predictions of the select models\n",
        "validation_predictions = {}\n",
        "test_predictions = {}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hfqm9F1k3Lc8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "805e6387-dba9-4fe7-e784-39ad5a1992f9"
      },
      "source": [
        "## Model1: Fully-Connected Neural Network\n",
        "def build_model_FC(hp):\n",
        "  model = keras.Sequential()\n",
        "  model.add(tf.keras.layers.InputLayer(input_shape=(133, 133, 3)))\n",
        "  model.add(tf.keras.layers.Rescaling(1./255))\n",
        "  model.add(tf.keras.layers.Flatten())\n",
        "  n_layers = hp.Int(\"num_layers\", 1, 5)\n",
        "  for i in range(n_layers):\n",
        "    model.add(tf.keras.layers.Dense(units=hp.Choice(\"units_\" + str(i), values=[16, 64, 256, 1024]), activation=\"relu\"))\n",
        "\n",
        "  model.add(tf.keras.layers.Dropout(hp.Float('dropout', min_value=0.0, max_value=0.99, default=0.3,step=0.05)))\n",
        "\n",
        "  model.add(keras.layers.Dense(6, activation='softmax')) # 6 classes (multiclass classification)\n",
        "\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(hp.Float(\"learning_rate\",min_value=1e-5, max_value=1,sampling=\"log\",default=1e-2)),\n",
        "      loss=\"categorical_crossentropy\",\n",
        "      metrics=[tf.keras.metrics.AUC(),\n",
        "               tf.keras.metrics.Recall(),\n",
        "               tf.keras.metrics.Precision()])\n",
        "  return model\n",
        "\n",
        "## Random Search tuner\n",
        "tuner = keras_tuner.RandomSearch(build_model_FC, objective=keras_tuner.Objective(\"val_auc\", direction=\"max\"), max_trials=10, executions_per_trial=1,overwrite=True)\n",
        "\n",
        "tuner.search(X_train, tf.keras.utils.to_categorical(y_train),\n",
        "             batch_size=64,epochs=20,validation_data=(X_val, tf.keras.utils.to_categorical(y_val)),\n",
        "             callbacks=[tf.keras.callbacks.EarlyStopping(monitor=\"val_auc\", patience=10)])\n",
        "\n",
        "## # of models we try\n",
        "n_model = 2\n",
        "best_models_FC = tuner.get_best_models(num_models=n_model)\n",
        "best_hyperparamters_FC = tuner.get_best_hyperparameters(num_trials=n_model)\n",
        "\n",
        "df_FC = None\n",
        "for i in range(n_model):\n",
        "    model = best_models_FC[i]\n",
        "    validation_predictions['FC_{}'.format(i)] = model.predict(X_val)\n",
        "    test_predictions['FC_{}'.format(i)] = model.predict(X_test)\n",
        "\n",
        "    d = collections.OrderedDict(sorted(best_hyperparamters_FC[i].values.items()))\n",
        "    df = pd.DataFrame.from_dict(d, orient='index', columns=['FC_{}'.format(i)])\n",
        "    df.loc['Loss'], df.loc['AUC'], _, _ = \\\n",
        "     model.evaluate(X_val, tf.keras.utils.to_categorical(y_val), verbose=0)\n",
        "    df_FC = pd.concat((df_FC, df), axis=1)\n",
        "\n",
        "df_FC.to_csv('df_FC.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 10 Complete [00h 00m 20s]\n",
            "val_auc: 0.5539094805717468\n",
            "\n",
            "Best val_auc So Far: 0.5656721591949463\n",
            "Total elapsed time: 00h 02m 05s\n",
            "INFO:tensorflow:Oracle triggered exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azBgcsfGNvWi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8265d609-6229-485f-f9c9-6ee4dfaf7462"
      },
      "source": [
        "### Model2: Convolutional Neural Network\n",
        "\n",
        "def build_model_CN(hp):\n",
        "  model = keras.Sequential()\n",
        "  model.add(tf.keras.layers.InputLayer(input_shape=(133, 133, 3)))\n",
        "  model.add(tf.keras.layers.Rescaling(1./255))\n",
        "  n_conv_layers = hp.Int(\"num_conv_layers\", 1, 4)\n",
        "  for i in range(n_conv_layers):\n",
        "     model.add(tf.keras.layers.Conv2D(filters=hp.Choice(\"filters_\" + str(i), values=[16, 32, 64]),\n",
        "             kernel_size=hp.Choice(\"kernel_size_\" + str(i),values=[3, 5]), activation='relu',padding='same'))\n",
        "     model.add(tf.keras.layers.BatchNormalization())\n",
        "     model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
        "     model.add(tf.keras.layers.Dropout(hp.Float('dropout_' + str(i), min_value=0.0, max_value=0.99,default=0.3, step=0.05)))\n",
        "  model.add(tf.keras.layers.Flatten())\n",
        "  model.add(keras.layers.Dense(6, activation='softmax'))\n",
        "  model.compile(\n",
        "      optimizer=tf.keras.optimizers.Adam(hp.Float(\"learning_rate\",min_value=1e-5,max_value=1,sampling=\"log\",default=1e-2)),loss=\"categorical_crossentropy\",\n",
        "      metrics=[\n",
        "               tf.keras.metrics.AUC(),\n",
        "               tf.keras.metrics.Recall(),\n",
        "               tf.keras.metrics.Precision()\n",
        "              ])\n",
        "  return model\n",
        "\n",
        "## Random Search tuner\n",
        "tuner = keras_tuner.RandomSearch(\n",
        "    build_model_CN,\n",
        "    objective=keras_tuner.Objective(\"val_auc\", direction=\"max\"),\n",
        "    max_trials=10,\n",
        "    executions_per_trial=1,\n",
        "    overwrite=True\n",
        ")\n",
        "\n",
        "tuner.search(\n",
        "             X_train, tf.keras.utils.to_categorical(y_train),\n",
        "             batch_size=64,\n",
        "             epochs=20,\n",
        "             validation_data=(X_val, tf.keras.utils.to_categorical(y_val)),\n",
        "             callbacks=[tf.keras.callbacks.EarlyStopping(\"val_auc\", patience=10)]\n",
        "             )\n",
        "\n",
        "## The number of models we keep\n",
        "n_model = 5\n",
        "best_models_CN = tuner.get_best_models(num_models=n_model)\n",
        "best_hyperparamters_CN = tuner.get_best_hyperparameters(num_trials=n_model)\n",
        "\n",
        "df_CN = None\n",
        "for i in range(n_model):\n",
        "    model = best_models_CN[i]\n",
        "\n",
        "    # validation_predictions['CN_{}'.format(i)] = np.argmax(model.predict(X_val), axis=1)\n",
        "    validation_predictions['CN_{}'.format(i)] = model.predict(X_val)\n",
        "    # test_predictions['CN_{}'.format(i)] = np.argmax(model.predict(X_test), axis=1)\n",
        "    test_predictions['CN_{}'.format(i)] = model.predict(X_test)\n",
        "    # print('X_test', model.predict(X_test))\n",
        "    # print('length X_test', len(model.predict(X_test)))\n",
        "\n",
        "    d = collections.OrderedDict(sorted(best_hyperparamters_CN[i].values.items()))\n",
        "    df = pd.DataFrame.from_dict(d, orient='index', columns=['CN_{}'.format(i)])\n",
        "    df.loc['Loss'], df.loc['AUC'], _, _ = \\\n",
        "     model.evaluate(X_val, tf.keras.utils.to_categorical(y_val), verbose=0)\n",
        "    df_CN = pd.concat((df_CN, df), axis=1)\n",
        "\n",
        "df_CN.to_csv('df_CN.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 10 Complete [00h 00m 05s]\n",
            "val_auc: 0.5569959282875061\n",
            "\n",
            "Best val_auc So Far: 0.5691357851028442\n",
            "Total elapsed time: 00h 01m 16s\n",
            "INFO:tensorflow:Oracle triggered exit\n",
            "WARNING:tensorflow:5 out of the last 20 calls to <function Model.make_test_function.<locals>.test_function at 0x7f1c8fb2dc20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 22 calls to <function Model.make_test_function.<locals>.test_function at 0x7f1c8f9925f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L759S1MIZ7Zg"
      },
      "source": [
        "## Save the validation and test predictions as .pckl files\n",
        "f = open('validation_predictions.pckl', 'wb')\n",
        "pickle.dump(validation_predictions, f)\n",
        "f.close()\n",
        "\n",
        "f = open('test_predictions.pckl', 'wb')\n",
        "pickle.dump(test_predictions, f)\n",
        "f.close()\n",
        "\n",
        "f = open('y_val.pckl', 'wb')\n",
        "pickle.dump(y_val, f)\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9m2cMg3xhLEc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}